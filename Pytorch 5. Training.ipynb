{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c596a142",
   "metadata": {},
   "source": [
    "<div style=\"display:block\" direction=rtl align=right><br><br>\n",
    "    <div  style=\"width:100%;margin:100;display:block\"  display=block align=center>\n",
    "        <img width=130 align=right src=\"https://i.ibb.co/yXKQmtZ/logo1.png\" style=\"margin:0;\" />\n",
    "        <img width=170 align=left  src=\"https://i.ibb.co/wLjqFkw/logo2.png\" style=\"margin:0;\" />\n",
    "        <span><br><font size=5>University of Tehran , school of ECE</font></span>\n",
    "        <span><br><font size=3>Deep Learning</font></span>\n",
    "        <span><br><font size=3>Spring 2023</font></span>\n",
    "    </div><br><br><br>\n",
    "    <div style=\"display:block\" align=left display=block> \n",
    "        <font size=3>Pytorch tutorial - Learning(Part2)</font><br>\n",
    "        <hr />\n",
    "        <font size=3>TA: <a href=\"mailto:farshads7778@gmail.com\">Farshad Sangari</a></font><br>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf78bcd-34ce-4dce-89c5-0c440480b663",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "260a8c5a-80f8-44c8-a545-2a7601df493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import  Dataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a42494-6cb6-4060-a47e-0f16da25b962",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Defined data on torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985196bc-6630-4779-8475-16401d1e195e",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ee0f92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_TRAIN = \"./data/CIFAR10/train/\"\n",
    "DIR_VAL = \"./data/CIFAR10/val/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acda8eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Classes:  10\n",
      "\n",
      "Total train images:  50000\n",
      "Total test images:  10000\n"
     ]
    }
   ],
   "source": [
    "classes = os.listdir(DIR_TRAIN)\n",
    "print(\"Total Classes: \", len(classes))\n",
    "\n",
    "train_imgs = []\n",
    "val_imgs  = []\n",
    "for _class in classes:\n",
    "    train_imgs += glob.glob(DIR_TRAIN + _class + '/*.jpg')\n",
    "    val_imgs += glob.glob(DIR_VAL + _class + '/*.jpg')\n",
    "\n",
    "print(\"\\nTotal train images: \", len(train_imgs))\n",
    "print(\"Total test images: \", len(val_imgs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bbffaf3",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b53470f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_transforms_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.49139968, 0.48215827 ,0.44653124), (0.24703233,0.24348505,0.26158768))])\n",
    "\n",
    "cifar_transforms_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.49139968, 0.48215827 ,0.44653124), (0.24703233,0.24348505,0.26158768))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05fd3bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Dataset(Dataset):\n",
    "    def __init__(self, imgs_list, classes, transforms=None):\n",
    "        super(CIFAR10Dataset, self).__init__()\n",
    "        self.imgs_list = imgs_list\n",
    "        self.class_to_int = {classes[i] : i for i in range(len(classes))}\n",
    "        self.transforms = transforms\n",
    "    def __getitem__(self, index):\n",
    "    \n",
    "        image_path = self.imgs_list[index]\n",
    "        \n",
    "        # Reading image\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Retriving class label\n",
    "        label = image_path.split(\"/\")[-2]\n",
    "        label = self.class_to_int[label]\n",
    "        \n",
    "        # Applying transforms on image\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(image)\n",
    "        return image, label\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a59ad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CIFAR10Dataset(imgs_list = train_imgs, classes = classes, transforms = cifar_transforms_train)\n",
    "val_dataset = CIFAR10Dataset(imgs_list = val_imgs, classes = classes, transforms = cifar_transforms_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0151ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True)\n",
    "\n",
    "cifar_val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be41564",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fd403d",
   "metadata": {},
   "source": [
    "### Custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "011bc563",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.MaxPool2d(2, 2) # output: 256 x 4 x 4\n",
    "            )\n",
    "        \n",
    "        self.feature_extractor[0].weight.data = torch.nn.init.xavier_normal_(self.feature_extractor[0].weight.data,\n",
    "                                                                     gain = torch.nn.init.calculate_gain(\"leaky_relu\"))\n",
    "        \n",
    "        ## Bias --> Standard distribution\n",
    "        self.feature_extractor[0].bias.data = torch.randn(self.feature_extractor[0].bias.data.shape)\n",
    "        \n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        # x = self.flat(x)\n",
    "        x = x.view(-1,256*4*4)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4b0266-e515-4c58-bda7-9c1ba5869890",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b578182-815b-4899-ae6e-73cd90cbec24",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe54ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"\n",
    "    Computes the accuracy over the k top predictions for the specified values of k\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            # correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            correct_k = correct[:k].float().sum()\n",
    "            res.append(correct_k.mul_(100.0 / batch_size).item())\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f8248c",
   "metadata": {},
   "source": [
    "### Simple training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "671d9b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f672563",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 10\n",
    "learning_rate = 1e-3\n",
    "gamma=0.5\n",
    "step_size=10\n",
    "ckpt_save_freq = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "cifar_train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=True)\n",
    "\n",
    "cifar_val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=False)\n",
    "\n",
    "\n",
    "model = ModelCNN().to(device)\n",
    "\n",
    "\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimzier\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "top1_acc_train = []\n",
    "top1_acc_val = []\n",
    "loss_avg_train = []\n",
    "loss_avg_val = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    sum_train_acc_epoch = 0\n",
    "    sum_val_acc_epoch = 0\n",
    "    sum_train_loss_epoch = 0\n",
    "    sum_val_loss_epoch = 0\n",
    "    \n",
    "    model.train()\n",
    "    mode = \"train\"\n",
    "    for batch_idx, (images, labels) in enumerate(cifar_train_loader,1):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels_pred = model(images)\n",
    "        loss = criterion(labels_pred, labels)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        acc1 = accuracy(labels_pred, labels)\n",
    "        sum_train_acc_epoch += acc1[0]\n",
    "        sum_train_loss_epoch += loss.detach().item()\n",
    "        if batch_idx % int(round(len(cifar_train_loader)/6)) ==0:\n",
    "            print(f\"At epoch {epoch}, average accuracy till batch_index --> {batch_idx}: {sum_train_acc_epoch/batch_idx}\")\n",
    "    top1_acc_train.append(sum_train_acc_epoch / batch_idx)\n",
    "    loss_avg_train.append(sum_train_loss_epoch / batch_idx)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    mode = \"val\"\n",
    "    with torch.no_grad():        \n",
    "        for batch_idx, (images, labels) in enumerate(cifar_val_loader,1):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            labels_pred = model(images)\n",
    "            loss = criterion(labels_pred, labels)\n",
    "            acc1 = accuracy(labels_pred, labels)\n",
    "            sum_val_acc_epoch += acc1[0]\n",
    "            sum_val_loss_epoch += loss.detach().item()\n",
    "\n",
    "        top1_acc_val.append(sum_val_acc_epoch / batch_idx)\n",
    "        loss_avg_val.append(sum_val_loss_epoch / batch_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62caa203",
   "metadata": {},
   "source": [
    "### Training utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebd41f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    computes and stores the average and current value\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, start_val=0, start_count=0, start_avg=0, start_sum=0):\n",
    "        self.reset()\n",
    "        self.val = start_val\n",
    "        self.avg = start_avg\n",
    "        self.sum = start_sum\n",
    "        self.count = start_count\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Initialize 'value', 'sum', 'count', and 'avg' with 0.\n",
    "        \"\"\"\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, num=1):\n",
    "        \"\"\"\n",
    "        Update 'value', 'sum', 'count', and 'avg'.\n",
    "        \"\"\"\n",
    "        self.val = val\n",
    "        self.sum += val * num\n",
    "        self.count += num\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def save_model(file_path, file_name, model, optimizer=None):\n",
    "    \"\"\"\n",
    "    In this function, a model is saved.Usually save model after training in each epoch.\n",
    "    ------------------------------------------------\n",
    "    Args:\n",
    "        - model (torch.nn.Module)\n",
    "        - optimizer (torch.optim)\n",
    "        - file_path (str): Path(Folder) for saving the model\n",
    "        - file_name (str): name of the model checkpoint to save\n",
    "    \"\"\"\n",
    "    state_dict = dict()\n",
    "    state_dict[\"model\"] = model.state_dict()\n",
    "\n",
    "    if optimizer is not None:\n",
    "        state_dict[\"optimizer\"] = optimizer.state_dict()\n",
    "    torch.save(state_dict, os.path.join(file_path, file_name))\n",
    "\n",
    "\n",
    "def load_model(ckpt_path, model, optimizer=None):\n",
    "    \"\"\"\n",
    "    Loading a saved model and optimizer (from checkpoint)\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(ckpt_path, map_location=torch.device(\"cpu\"))\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    if (optimizer != None) & (\"optimizer\" in checkpoint.keys()):\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"\n",
    "    Computes the accuracy over the k top predictions for the specified values of k\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            # correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            correct_k = correct[:k].float().sum()\n",
    "            res.append(correct_k.mul_(100.0 / batch_size).item())\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff660314-1689-42b0-bf31-a9e0a7b342d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    model,\n",
    "    model_name,\n",
    "    epochs,\n",
    "    learning_rate,\n",
    "    gamma,\n",
    "    step_size,\n",
    "    device,\n",
    "    load_saved_model,\n",
    "    ckpt_save_freq,\n",
    "    ckpt_save_path,\n",
    "    ckpt_path,\n",
    "    report_path,\n",
    "):\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # optimzier\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if load_saved_model:\n",
    "        model, optimizer = load_model(\n",
    "            ckpt_path=ckpt_path, model=model, optimizer=optimizer\n",
    "        )\n",
    "\n",
    "    lr_scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    report = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"model_name\",\n",
    "            \"mode\",\n",
    "            \"image_type\",\n",
    "            \"epoch\",\n",
    "            \"learning_rate\",\n",
    "            \"batch_size\",\n",
    "            \"batch_index\",\n",
    "            \"loss_batch\",\n",
    "            \"avg_train_loss_till_current_batch\",\n",
    "            \"avg_train_top1_acc_till_current_batch\",\n",
    "            \"avg_val_loss_till_current_batch\",\n",
    "            \"avg_val_top1_acc_till_current_batch\"])\n",
    "\n",
    "    for epoch in tqdm(range(1, epochs + 1)):\n",
    "        top1_acc_train = AverageMeter()\n",
    "        loss_avg_train = AverageMeter()\n",
    "        top1_acc_val = AverageMeter()\n",
    "        loss_avg_val = AverageMeter()\n",
    "\n",
    "        model.train()\n",
    "        mode = \"train\"\n",
    "        \n",
    "        \n",
    "        loop_train = tqdm(\n",
    "            enumerate(train_loader, 1),\n",
    "            total=len(train_loader),\n",
    "            desc=\"train\",\n",
    "            position=0,\n",
    "            leave=True)\n",
    "        for batch_idx, (images, labels) in loop_train:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            labels_pred = model(images)\n",
    "            loss = criterion(labels_pred, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            acc1 = accuracy(labels_pred, labels)\n",
    "            top1_acc_train.update(acc1[0], images.size(0))\n",
    "            loss_avg_train.update(loss.item(), images.size(0))\n",
    "\n",
    "            new_row = pd.DataFrame(\n",
    "                {\"model_name\": model_name,\n",
    "                 \"mode\": mode,\n",
    "                 \"image_type\":\"original\",\n",
    "                 \"epoch\": epoch,\n",
    "                 \"learning_rate\":optimizer.param_groups[0][\"lr\"],\n",
    "                 \"batch_size\": images.size(0),\n",
    "                 \"batch_index\": batch_idx,\n",
    "                 \"loss_batch\": loss.detach().item(),\n",
    "                 \"avg_train_loss_till_current_batch\":loss_avg_train.avg,\n",
    "                 \"avg_train_top1_acc_till_current_batch\":top1_acc_train.avg,\n",
    "                 \"avg_val_loss_till_current_batch\":None,\n",
    "                 \"avg_val_top1_acc_till_current_batch\":None},index=[0])\n",
    "\n",
    "            \n",
    "            report.loc[len(report)] = new_row.values[0]\n",
    "            \n",
    "            loop_train.set_description(f\"Train - iteration : {epoch}\")\n",
    "            loop_train.set_postfix(\n",
    "                loss_batch=\"{:.4f}\".format(loss.detach().item()),\n",
    "                avg_train_loss_till_current_batch=\"{:.4f}\".format(loss_avg_train.avg),\n",
    "                top1_accuracy_train=\"{:.4f}\".format(top1_acc_train.avg),\n",
    "                max_len=2,\n",
    "                refresh=True,\n",
    "            )\n",
    "        if epoch % ckpt_save_freq == 0:\n",
    "            save_model(\n",
    "                file_path=ckpt_save_path,\n",
    "                file_name=f\"ckpt_{model_name}_epoch{epoch}.ckpt\",\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "            )\n",
    "\n",
    "        model.eval()\n",
    "        mode = \"val\"\n",
    "        with torch.no_grad():\n",
    "            loop_val = tqdm(\n",
    "                enumerate(val_loader, 1),\n",
    "                total=len(val_loader),\n",
    "                desc=\"val\",\n",
    "                position=0,\n",
    "                leave=True,\n",
    "            )\n",
    "            for batch_idx, (images, labels) in loop_val:\n",
    "                optimizer.zero_grad()\n",
    "                images = images.to(device).float()\n",
    "                labels = labels.to(device)\n",
    "                labels_pred = model(images)\n",
    "                loss = criterion(labels_pred, labels)\n",
    "                acc1 = accuracy(labels_pred, labels)\n",
    "                top1_acc_val.update(acc1[0], images.size(0))\n",
    "                loss_avg_val.update(loss.item(), images.size(0))\n",
    "                new_row = pd.DataFrame(\n",
    "                    {\"model_name\": model_name,\n",
    "                     \"mode\": mode,\n",
    "                     \"image_type\":\"original\",\n",
    "                     \"epoch\": epoch,\n",
    "                     \"learning_rate\":optimizer.param_groups[0][\"lr\"],\n",
    "                     \"batch_size\": images.size(0),\n",
    "                     \"batch_index\": batch_idx,\n",
    "                     \"loss_batch\": loss.detach().item(),\n",
    "                     \"avg_train_loss_till_current_batch\":None,\n",
    "                     \"avg_train_top1_acc_till_current_batch\":None,\n",
    "                     \"avg_val_loss_till_current_batch\":loss_avg_val.avg,\n",
    "                     \"avg_val_top1_acc_till_current_batch\":top1_acc_val.avg},index=[0],)\n",
    "                \n",
    "                report.loc[len(report)] = new_row.values[0]\n",
    "                loop_val.set_description(f\"val - iteration : {epoch}\")\n",
    "                loop_val.set_postfix(\n",
    "                    loss_batch=\"{:.4f}\".format(loss.detach().item()),\n",
    "                    avg_val_loss_till_current_batch=\"{:.4f}\".format(loss_avg_val.avg),\n",
    "                    top1_accuracy_val=\"{:.4f}\".format(top1_acc_val.avg),\n",
    "                    refresh=True,\n",
    "                )\n",
    "        lr_scheduler.step()\n",
    "    report.to_csv(f\"{report_path}/{model_name}_report.csv\")\n",
    "    return model, optimizer, report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b4bf96e-f208-4f7f-882e-763045998550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train - iteration : 1:   4%|▎         | 29/782 [00:11<05:05,  2.46it/s, avg_train_loss_till_current_batch=2.3594, loss_batch=2.2965, max_len=2, top1_accuracy_train=10.3448]\n",
      "  0%|          | 0/10 [00:11<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22536/3855824817.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                                 shuffle=False)\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m trainer = train(\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcifar_train_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcifar_val_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22536/4065749013.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, val_loader, model, model_name, epochs, learning_rate, gamma, step_size, device, load_saved_model, ckpt_save_freq, ckpt_save_path, ckpt_path, report_path)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mlabels_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22536/693150001.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;31m# x = self.flat(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 10\n",
    "learning_rate = 1e-3\n",
    "gamma=0.5\n",
    "step_size=10\n",
    "ckpt_save_freq = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "custom_model = ModelCNN()\n",
    "\n",
    "cifar_train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=True)\n",
    "\n",
    "cifar_val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=False)\n",
    "\n",
    "trainer = train(\n",
    "    train_loader=cifar_train_loader,\n",
    "    val_loader=cifar_val_loader,\n",
    "    model = custom_model,\n",
    "    model_name=\"Custom model\",\n",
    "    epochs=epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    gamma = gamma,\n",
    "    step_size = step_size,\n",
    "    device=device,\n",
    "    load_saved_model=False,\n",
    "    ckpt_save_freq=ckpt_save_freq,\n",
    "    ckpt_save_path=\"./\",\n",
    "    ckpt_path=\"./\",\n",
    "    report_path=\"./\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9dea8ac6c336defc73203621829f420c5c8cb20f705d9a92119e91f16d7af95c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
