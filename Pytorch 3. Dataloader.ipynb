{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f7af959",
   "metadata": {},
   "source": [
    "<div style=\"display:block\" direction=rtl align=right><br><br>\n",
    "    <div  style=\"width:100%;margin:100;display:block\"  display=block align=center>\n",
    "        <img width=130 align=right src=\"https://i.ibb.co/yXKQmtZ/logo1.png\" style=\"margin:0;\" />\n",
    "        <img width=170 align=left  src=\"https://i.ibb.co/wLjqFkw/logo2.png\" style=\"margin:0;\" />\n",
    "        <span><br><font size=5>University of Tehran , school of ECE</font></span>\n",
    "        <span><br><font size=3>Deep Learning</font></span>\n",
    "        <span><br><font size=3>Spring 2023</font></span>\n",
    "    </div><br><br><br>\n",
    "    <div style=\"display:block\" align=left display=block> \n",
    "        <font size=3>Pytorch tutorial - Dataloader</font><br>\n",
    "        <hr />\n",
    "        <font size=3>TA: <a href=\"mailto:farshads7778@gmail.com\">Farshad Sangari</a></font><br>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf78bcd-34ce-4dce-89c5-0c440480b663",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "260a8c5a-80f8-44c8-a545-2a7601df493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985196bc-6630-4779-8475-16401d1e195e",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75da4798",
   "metadata": {},
   "source": [
    "#### Method1 - torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41416513-4818-4689-bb74-2ac93ab7e2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_transform = transforms.Compose(\n",
    "                    [transforms.RandomRotation(10),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "mnist_val_transform = transforms.Compose(\n",
    "                    [transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307,), (0.3081,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e701566-a096-4fbd-a4ed-b693a8e583a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "mnist_train_set = torchvision.datasets.MNIST(root='./data',\n",
    "                                       train=True,\n",
    "                                       download=True,\n",
    "                                       transform=mnist_train_transform)\n",
    "\n",
    "mnist_val_set = torchvision.datasets.MNIST(root='./data',\n",
    "                                       train=False,\n",
    "                                       download=True,\n",
    "                                       transform=mnist_val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f7e6236",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_loader = torch.utils.data.DataLoader(mnist_train_set,\n",
    "                                                batch_size=16,\n",
    "                                                shuffle=True)\n",
    "\n",
    "mnist_val_loader = torch.utils.data.DataLoader(mnist_val_set,\n",
    "                                                batch_size=16,\n",
    "                                                shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71ac5a9e",
   "metadata": {},
   "source": [
    "#### Method2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "968f1369",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_TRAIN = \"./data/CIFAR10/train/\"\n",
    "DIR_VAL = \"./data/CIFAR10/val/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e43346be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Classes:  10\n",
      "\n",
      "Total train images:  50000\n",
      "Total test images:  10000\n"
     ]
    }
   ],
   "source": [
    "classes = os.listdir(DIR_TRAIN)\n",
    "print(\"Total Classes: \", len(classes))\n",
    "\n",
    "train_imgs = []\n",
    "val_imgs  = []\n",
    "for _class in classes:\n",
    "    train_imgs += glob.glob(DIR_TRAIN + _class + '/*.jpg')\n",
    "    val_imgs += glob.glob(DIR_VAL + _class + '/*.jpg')\n",
    "\n",
    "print(\"\\nTotal train images: \", len(train_imgs))\n",
    "print(\"Total test images: \", len(val_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacf185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_transforms_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.49139968, 0.48215827 ,0.44653124), (0.24703233,0.24348505,0.26158768))])\n",
    "\n",
    "cifar_transforms_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.49139968, 0.48215827 ,0.44653124), (0.24703233,0.24348505,0.26158768))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4c842a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, imgs_list, classes, transforms=None):\n",
    "        super(CIFAR10Dataset, self).__init__()\n",
    "        self.imgs_list = imgs_list\n",
    "        self.class_to_int = {classes[i] : i for i in range(len(classes))}\n",
    "        self.transforms = transforms\n",
    "    def __getitem__(self, index):\n",
    "    \n",
    "        image_path = self.imgs_list[index]\n",
    "        \n",
    "        # Reading image\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Retriving class label\n",
    "        label = image_path.split(\"/\")[-2]\n",
    "        label = self.class_to_int[label]\n",
    "        \n",
    "        # Applying transforms on image\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(image)\n",
    "        return image, label\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5d9adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CIFAR10Dataset(imgs_list = train_imgs, classes = classes, transforms = cifar_transforms_train)\n",
    "val_dataset = CIFAR10Dataset(imgs_list = val_imgs, classes = classes, transforms = cifar_transforms_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1289a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                batch_size=16,\n",
    "                                                shuffle=True)\n",
    "\n",
    "cifar_val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                                batch_size=16,\n",
    "                                                shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c06fd4a",
   "metadata": {},
   "source": [
    "#### Implementation torchvision datasets using method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "281afa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self,mode, transforms=None):\n",
    "        super(MNISTDataset, self).__init__()\n",
    "        self.transforms = transforms\n",
    "        is_train = True if mode == \"train\" else False\n",
    "        self.data = torchvision.datasets.MNIST(\n",
    "                                       root='./data',\n",
    "                                       train=is_train,\n",
    "                                       download=True)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "    \n",
    "        image, label = self.data[index]\n",
    "        \n",
    "        # Applying transforms on image\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "fb8b6d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_train = MNISTDataset(mode = \"train\" , transforms=mnist_train_transform)\n",
    "MNIST_val = MNISTDataset(mode = \"val\" , transforms=mnist_val_transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11db2c8b",
   "metadata": {},
   "source": [
    "## Imshow CIFAR images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "da660089",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(cifar_val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "cdf98b0b-d81e-461f-8755-3eb4254e6b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD7CAYAAABDsImYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAVuUlEQVR4nO2dyY9c53XFb9WrV3N1VbO7OTRFUqREcaZEUTYlSowQG3EM2OvA8SKLLAIlf0hW2QfIIkgMOJYVQZ4kQSE0kZQlUYxM2pwkTiYpdavnququrvG9elkk2eWcqkAIguSe3/aw3vv6e+/UB/DUvTeVJEliQoj/16T/txcghPifR0YXwgEyuhAOkNGFcICMLoQDZHQhHCCjC+EAGV0IB8joQjggM+4//Ou/+Tuo7d+/H2pBkKLXzeZCqG1uNqF2/8E9qPU6LXrPjbUVqNVXl6AWJEOoPbx7G2rlPP4bzcwe3L6FxcEAX7dQgFqj3oDaai9L19PptqEWkfVE5JqjThSm1wpYnZioQG1uEb8/IX8t6d9SqeSgNjk9DbXmOn8v1xobUCsVi1B72MSf+090ogvhABldCAfI6EI4QEYXwgEyuhAOkNGFcMDY8VqtMgG1mZkpqJXLZXrdIIO/a9bWcCxVX12G2nJ3k95zEMVQ6/R6UDt14imoPbpzB9TOvvkGXU86g+OuycktUOtv4r8zn89DLTvEMaGZmSV4PVEK790gJtfFHzMzs5DkXSGJEVMZ/I4USwHUkpj3W0mRPcqQ5xUSbcsUfpZmZmGIn1mvj9/LcdCJLoQDZHQhHCCjC+EAGV0IB8joQjhARhfCAWPHaxNVHJNNTU5CrTbNI4V0isUcuIaoWq1Crd7A1WlmZonhKKdWrUFteus2qD0yux1qn9+4QdezazuO5liM+Omli1CrkGfSaOMKPTOzbBZ//8cBiYCiPtRGJXphFsdSGRKhRUP8LLs9fNPKiNi3OlWDWjqN96fTx3uQSvGSuVQKr3dzc3SFGkMnuhAOkNGFcICMLoQDZHQhHCCjC+EAGV0IB8joQjhg/Bx9ApepZrM45yyTckkzs4QErJVSCWsV3P0zzOAunWZmYUi6ssY4z+0NcK5fKuBc9lvf+WO6nif27YXapxc/gdrNz3D32IRkttUcLt80M0uT0uFhgl+Z7gBrvRFlqkmKlJQG+JkM0/ie07M7oXbs2JN0PUePHoFavV6H2uVP8fNqNtboPXN5/N4Ws9xHo9CJLoQDZHQhHCCjC+EAGV0IB8joQjhARhfCAf+NeA3HWaxsLxPw75I0KYksksFyrMtpFLEReWbs+20Qkbivgktj86RT6e7ju+lq1us4dtns4rLH0gQuRV1dxqWo26fxvpqZJSmyPwmO7To9XHLc7PAuph3yyIIcfta5Io5gjz31NNSeeeYZup4DBw5A7fbNa1D7/NZnUNtYb9B7phKcQZIUcSx0ogvhABldCAfI6EI4QEYXwgEyuhAOkNGFcMDY/2mfz+EKosEARycjmn9alqwgk8FikuAoJx4Rr7Fo7shhHKvs3rMH37PHYjBc+Wdm9tFHH0Htg48vQW1xDcdyORJNTuYGdD3tLt6/hAxSZAMzLRjRAXWIn3V5C+4kvP2RfVB75rkzUJudnaXrqW+0oXbn/hf4c40m1CIy3NPMLOp1oNbe4INDR6ETXQgHyOhCOEBGF8IBMroQDpDRhXCAjC6EA8aO1wpFXJ3Fq8V41JUiZTlpwxFaLoPjvpmZGXrPmS1PQO3k0yehliJh4eLiPNTm5hfoejI53BRweutWqLW7XaixxpqZDdzc0MwsGuAop0+K0JIMfkfCED8vM7OghCPIxw7gRo0nvnkaavsOHIbagAxDNDO7eh1XqF2+/DuoNZs4XitkeFNOG+BzN5/7emeyTnQhHCCjC+EAGV0IB8joQjhARhfCATK6EA6Q0YVwwNg5eqmMM9J+D4eradJR1Ix/07BhiLXJGtQOPL6f3vPYMZzLZsk9Y1KO26jj/Tl/+X26nsMHD0HtwJ/jstkL5y9A7fqN61BLbeLfJ5jx30X0SZlqmMedggsFnOubmZUmt0Ht4JGjUHvhzItQW1hegVp9fYOu59ade1D77DYebmndFpSmq/gdMTNLBjjbz2Z4me8odKIL4QAZXQgHyOhCOEBGF8IBMroQDpDRhXDA2PFaHOMOlgHp1poiAxjNzIakTexggLuVVstlqG0lXUPNzLbUcEnkjWt4SN67770NtY8//ABqo4Y+JkOsf/9734faS3/5EtRu3rwJtauv/y1dzzvvnYdauYhLaoMC3teIlGCamf3JD38ItUMnTkFts4sjz0IJd8K9cP4cXc/Zs29BbW3pK6idOIRLoKsFvgfz93Gk12nzDrKj0IkuhANkdCEcIKML4QAZXQgHyOhCOEBGF8IBY8drZTIocHMDVwJ1u3hwnJlZllTFFUh31PlGA2pXLn9K79lu4wF6F869B7WYVBcVSOxUKfMhiwtzc1D75GM8gPHZU89C7fhhXKF3oPJndD3Tu/Dgwn/48U+h1iNDBP/ouzgmNDM7dfo5qK2u4wgtCHBn1eukW+uvL/B4bX0VD7Dcs3Mn1EoFbKnWRoPek82hrFXGtup/iU50IRwgowvhABldCAfI6EI4QEYXwgEyuhAOGPv/7LNkSB5rszcc8qobVtxWIFEFG3j45YOH9J63b+EKtTSp0quSqred23dALRPy79PllWWofXbzKtQKebw/R47geC0d8eaQB5/GUdd3Wnjfl+r4TXjh29+i90xItJRK4fVurjegdu69s1C7evkSXU+YxpWTu3duh1o2hT/XjvhgRxYnD2M8UHMcdKIL4QAZXQgHyOhCOEBGF8IBMroQDpDRhXDA+CUxpLSGzUhj2ihIz0TL5/NQ2zmL4w8zs4zhCK3dwhHRMMYLGvZxhVWniyMXM7NkgKOT7gBX/z24cxtqURdX6BWqeO/MzC5d+g3U5hYWodZc34Ta/nsP6D2Li3Wo7dqzF2pvv/UG1C6Rhp2FDI8Yd+zEcWk5T85H8s5WSaWmmVmPvJfNOn+HRqETXQgHyOhCOEBGF8IBMroQDpDRhXCAjC6EA2R0IRwwdo4ehPifZohG61DNjETTFg+wGBjO9ScnKvSemeE2qC3O4yxzGOOsvL25DrXWepOuJxrg64ZZ3OV0YwN3Kl1YwHu3vsL3560PLkJtQJ7JyZPPQK25iTN2M7OFFfy33CfDB998/WdQW1ueh9qR/TibNzPbOjkJtWCIy01zWVJaneW/KWn18HuQyZA63jHQiS6EA2R0IRwgowvhABldCAfI6EI4QEYXwgFfb3LbfzBMcMlfxPIzMzNSxprP4eUV87hj5qggorPZgtrUFO70GvdwOemgi8tbA1J+aGYWkm6u7I/pbjag1iBdQxcLPOZ57Pg3oDZRwtHcX730F1DbaOIyVDOzi7/GJaU//se/h9q9O3ehViviv7NSxF2NzcxyadztNiTPpJTD92zVeRfYASlXnqxV6WdHoRNdCAfI6EI4QEYXwgEyuhAOkNGFcICMLoQDxo7Xhin8nTA0HK+xTq5mZuSyfABjCXfUzJIKIjMzS3DctbqIq6j6XVyB1VjDgxLjmHfwLGRLUItIpVQmgyvbdsxMQ23qIB6iaGa2srwEtV4LR5MrTVzBV8nxDqiPP/441CbKZawV8bPeuX0Gammyr2ZmAbFGjgxD7HXIyNEhjuzMzKqk6jIM8bMeB53oQjhARhfCATK6EA6Q0YVwgIwuhANkdCEcMHa8xmYlZkncwJpKmplFpLCLfTImHwwDfs8yiWsW5x7i9ZCvxWoVVxelUzxW6fdx1dImiax2kGGSTz11HGq/DXCFnplZvLwKtekdePjghx9/ArWTxw7Rez6yaxfUfvCnP4Da66SUrLvegFqaTUM0szypQmPvweo6fl7ZHI/IKqUpqC0t4fh2HHSiC+EAGV0IB8joQjhARhfCATK6EA6Q0YVwwNjxWmmI50IlJHqLRjRG7JEPJ6yybQJXJhWmZ+k9F65dhVqWZCdZwzFYJ8aVbctNPnutH+ShtuvE81B76vk/hNrUwSNQ27u4Qtcz7OEKrPs3r0BtY+kLqF2t36L3LDx5FGpHZ4tQ6z93DGrnzl2AWq6AZ6uZmcUpMvOuhZ/1MIObTmbSvG1pt4+rHPOkGeo46EQXwgEyuhAOkNGFcICMLoQDZHQhHCCjC+EAGV0IB4ydo6fTLFfEA/QGQ/5dMjWNM2RW1LfewV08NzZIJ04zCwJ85U1S/ppK4263CfnODEKegc7u2A21U8+dhtqRk3gYYkLy3D0h78ja32hA7f6130Dt5tVrUPsi4GWhlQBnzMUQ7+2V316BWr2OO/qWY55p5/N4j1Jk8mVASqQT0n3YzCwi716/z7vWjkInuhAOkNGFcICMLoQDZHQhHCCjC+EAGV0IB4wfr5HvhFwGX6bf45ECDqzMBqR56tIK7oq50WrTexZLeJhdMIPLXxe+vIsvGuHF7j94mK7n8MlnofbkN7CWzuIIbaWN46wUa71rZl89fAC1q1cuQ+3zazhemy6RWmYzu70dd0ANE/yW3CCRXkSeiRlfD5uHGJB4jQ3wTIZ834cjhjB+HXSiC+EAGV0IB8joQjhARhfCATK6EA6Q0YVwwNjxGqNcwgMGY8MdM83M1ptYj2Mcq/S6HagFWR6dTEzhKGe5g4fkxUN83T379kHt29/9Hl3P7icOQq2fxdV96x0coTXJsL+bV67Q9fzin1+F2oW334FaJcDx0LYpPBDSzKyz3oLawgruWrtex92JSfpovR7u6GtmliUVhwmpfmTxWjrh8VkqhWO7kE05HQOd6EI4QEYXwgEyuhAOkNGFcICMLoQDZHQhHDB2vNbr4ygnS3KMqMkjheXlBXzdHB6uVyxgrVDgzQ9bdRyBrDVwY8kDx09A7fRpXGW27+hxuh72GFqkhG+tiSO0+flFqL3189foai6+fx5qbRKHHj32KNT2zO6g90xHOCbrd3E1YimPI6mIlEamWQWamWVIRSaLuoYxfl7DAY6EzXj1WkIq+MZBJ7oQDpDRhXCAjC6EA2R0IRwgowvhABldCAfI6EI4YOwcPZXC3wlp8nXRbfOOrJtkIGJAskxW0jdqyOLS8irUHiMdW58//U2ozc7uhNrKapOuJ1MsQy1LfhPw8OGXUHv1tZ9D7e3Xf0XX017DHXYf3TYJtZ1TuIPuqCLLMMD/YtvUNNRS5GcacwtLUCuQ32GY8d9p5PK4dHjQw78HaEe8NDYe4kGKw4gPqRyFTnQhHCCjC+EAGV0IB8joQjhARhfCATK6EA4YO15jA+s6mzhC67R4F9iU4esOujiOWF7GEdA66YBqZjb7yCNQe/HMGahNTuJut3NLuCx0ud6g69m6A3cVXf0SR0S/JBHaKy//FGr9+S/oeiZyuOx411YcoU3kSOfUPo6dzMyyeXzP4kQNakPSKbhP6lTzZNCmmVl+RPyGiMkgxTgeMXCUlKJmMqTz7BjoRBfCATK6EA6Q0YVwgIwuhANkdCEcIKML4YCx47XVZTzortPBMdhmi1eS5QK8hEEXD95bnH8ItZBN1zOzMy++CLWZbVugNjeHo647D3BkVa3hoY5mZr0Brkx64403oPbaq69AbWNxHmozWR7VlLP4meRJqSL5mIUjmpjGEa7cSkhkFfXw52ZmtuEbBvwdIamddciAzzap1uyTyjYzs0yA97aQwxVz46ATXQgHyOhCOEBGF8IBMroQDpDRhXCAjC6EA8aO1+7dvgu1WnUCarOk2snM7Pr161BbazagNjmBGyq+cOYP6D1r0zWo9chQwzappnvi0BF6T8bLP/kJ1P7pRz+CWmMJR2iZNM6HpkkzSjOzchE3pMyRNyZIcAxWIJVtZmaDDo6smps4oo3JYMItFfxero2oqtwgepzge6ZJ/BiQKNnMLCHNI3sJH9A4Cp3oQjhARhfCATK6EA6Q0YVwgIwuhANkdCEcMHa8liFz0Eol3Ejv5o1r9LrzD+9Dbd++fVA7dBTPSAtH/FXs2y0XYnWaVEPVm7gh5ScXP6LrOfvWm1C7ewPHj4M+joDyZBNqFR551iZwvDZVxU0VcyGuihuMaA7ZaeO/JSGxXY7EdmmyB8MhL6fr9wdQi8l6AtLgkUVvZmapAO9fOo1nDY6DTnQhHCCjC+EAGV0IB8joQjhARhfCATK6EA6Q0YVwwNg5er+Lc9DF+a+gdvvmDX7dHu6a2W5NQm3HDMmCyZBAM7OIlDYOyXffIMLZ6pXLl6H2yssv0/VcePcdqMV9vD+1PM5dp0gZL8vJzcwmSVZemcDdSIMUKfHt8LLQLvlNQJjFv9Mol/Fah+Q5D/q4e6yZWYd0bE3IsMQwwHl3mmTsZmYpIgdf80zWiS6EA2R0IRwgowvhABldCAfI6EI4QEYXwgHjD1lcWoba3Jf3oRZ1eaySzeLvms9/dwVqe3fNQu3IqefoPbvruKS01cORzJWruOT2V7/4GdQ+eB/HZ2ZmEYkYt+Tw/sxuwV1Op2o4XiuV+cC+QomUfpLusjGJH/tDrP37dXEslcvjuDRXwFHhSgN3j+0MeLwWkQjN8CtiaSbGfA8yZPZlmFG8JoQYgYwuhANkdCEcIKML4QAZXQgHyOhCOGDseG1leRFqS4tY2zZVpdeNOi2o1VeWoPb+O2ehFpBupGZm2TJe02e/n4Pam2dxTPbu2/8CtcYajibNeBXaZBFHS+UAR0CTeRxX1Sb5kMUs6fg7JB1QhzGu+Mqw7MjMwhBXqGWy+LNRHEFtvYXjtWhA4jMzy2XxvrMqsxSL0Ib8XA3Je1vM8yGVo9CJLoQDZHQhHCCjC+EAGV0IB8joQjhARhfCAWPHa+v1JtSqpRLUok6HXndtBTeWnKziGOjB3VtQe/OXXXrPvYeO48++ew5q75//GGrzXzyEWn5EtFQr4mqyQhpXWVULOALaXsXPJFvDVW9mZvEAx2T9Lq60S5GoK8zxPWBDPKMY51mNZgNqPdbgMeFnXDaL46wMGZaYDMh1+RZYMR9CrVLhkegodKIL4QAZXQgHyOhCOEBGF8IBMroQDpDRhXCAjC6EA8bP0Tdw59Qyyf/WGrxEMyKZbSrBpYvDGOfLVy7/K73n7+cXoPbRh59A7d6d++SquPtntYQHAZqZ5clTmCbDEh+d3Qa1bTW8dw1cwWpmZsMhLuHs9/HvIlIx3oOQlH2amaXTOGTutXF2v7aGy5zjhJS3DvnAwzQ5A1mOnslgLwSkg64ZL43NhypTFUKMQEYXwgEyuhAOkNGFcICMLoQDZHQhHDB2vLaxgTtqtpo46hp28efMzLZP4fhoaRGXsIYBzog6bT7Y8e6lS/izLRwfhSH+XszncJxVIuWkZmYZw51DZ6a2QG126zS+ZoxLdbt9Xsbb7+I9YKWfAesQS4ZFmpnFJLLqkoGIrQ6O3sIcfreiiHeBTcjfwrrAFkgnVxa9mZllSMSYSo3IREegE10IB8joQjhARhfCATK6EA6Q0YVwgIwuhANSSZLwkhohxP95dKIL4QAZXQgHyOhCOEBGF8IBMroQDpDRhXCAjC6EA2R0IRwgowvhgH8DmROvP/y1IaQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def plot_input_sample(batch_data,\n",
    "                      mean = [0.49139968, 0.48215827 ,0.44653124],\n",
    "                      std = [0.24703233,0.24348505,0.26158768],\n",
    "                      to_denormalize = False,\n",
    "                      figsize = (3,3)):\n",
    "    \n",
    "    batch_image,_ = batch_data\n",
    "    batch_size = batch_image.shape[0]\n",
    "    \n",
    "    random_batch_index = random.randint(0,batch_size-1)\n",
    "    random_image = batch_image[random_batch_index]\n",
    "    \n",
    "    image_transposed = random_image.detach().numpy().transpose((1, 2, 0))\n",
    "    if to_denormalize:\n",
    "        image_transposed = np.array(std)*image_transposed + np.array(mean)\n",
    "        image_transposed = image_transposed.clip(0,1)\n",
    "    fig, ax = plt.subplots(1, figsize=figsize)\n",
    "    ax.imshow(image_transposed)\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    \n",
    "     \n",
    "sample = next(iter(cifar_val_loader))\n",
    "\n",
    "plot_input_sample(batch_data=sample,\n",
    "                  mean = [0.49139968, 0.48215827 ,0.44653124],\n",
    "                  std = [0.24703233,0.24348505,0.26158768],\n",
    "                  to_denormalize = True,\n",
    "                  figsize = (3,3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
